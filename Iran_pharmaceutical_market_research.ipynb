{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38be22be",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Introduction to PySpark\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>Setup your working environment (Conda package management is recommended as it replace both pip and virtual environment)</li>\n",
       "            <li>I used WSL 2 for setting up a linux subsystem on my windows 11 in order to use GPU acceleration power.</li>\n",
       "            <li>You can install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
       "            <br> Don't forget to install Java SE 8 or above</li>\n",
       "            <li>I highly recommend to install spark at first, \n",
       "                then add \"SPARK_HOME\" environment variable in your ~/.bashrc and reload using \"source ~/.bashrc\", then install pyspark with conda.</li>\n",
       "            <li>If you want to open excel files, it's possible to open them with pandas with installed openpyxl as its dependency, then convert the file to csv, <br>\n",
       "                or instead there is an excel extension for spark 3.5.0 at the moment i work with the files. if your spark and scala version supports the jar file, open the excel file directly with spark.</li>\n",
       "        </ul>\n",
       "        <div>\n",
       "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
       "            I used Spark version 3.5.1 (Scala 2.13 & CUDA 12.5) and PySpark worked with python 3.11.9 well, earlier python version encountered with some errors during some code executions.\n",
       "        </div>\n",
       "    </P>\n",
       "</div>\n",
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Dataset\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>\n",
       "                 Previously used: Medicare Part D Prescribers - by Provider and Drug |\n",
       "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
       "            </li>\n",
       "            <li>\n",
       "                New dataset is available in datasets directory. which is real data of overall sales of medicines from different distributors and manufactures of Iran.\n",
       "            </li>\n",
       "        </ul>\n",
       "    </P>\n",
       "\n",
       "    <p>\n",
       "        <label>\n",
       "            <strong>\n",
       "                CUDA\n",
       "            </strong>\n",
       "        </label>\n",
       "        <div>\n",
       "            For GPU Accelerated tasks: <br>\n",
       "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
       "            <br>\n",
       "            Then download supported spark-rapids jar file and add it to spark plugins. <br>\n",
       "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
       "            Spark rapids built at the top of cuDF, by installing them you can leverage GPU acceleration for Pandas as well without any code change.\n",
       "            <ul>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
       "            </ul>\n",
       "        </div>\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Introduction to PySpark\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>Setup your working environment (Conda package management is recommended as it replace both pip and virtual environment)</li>\n",
    "            <li>I used WSL 2 for setting up a linux subsystem on my windows 11 in order to use GPU acceleration power.</li>\n",
    "            <li>You can install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
    "            <br> Don't forget to install Java SE 8 or above</li>\n",
    "            <li>I highly recommend to install spark at first, \n",
    "                then add \"SPARK_HOME\" environment variable in your ~/.bashrc and reload using \"source ~/.bashrc\", then install pyspark with conda.</li>\n",
    "            <li>If you want to open excel files, it's possible to open them with pandas with installed openpyxl as its dependency, then convert the file to csv, <br>\n",
    "                or instead there is an excel extension for spark 3.5.0 at the moment i work with the files. if your spark and scala version supports the jar file, open the excel file directly with spark.</li>\n",
    "        </ul>\n",
    "        <div>\n",
    "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
    "            I used Spark version 3.5.1 (Scala 2.13 & CUDA 12.5) and PySpark worked with python 3.11.9 well, earlier python version encountered with some errors during some code executions.\n",
    "        </div>\n",
    "    </P>\n",
    "</div>\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Dataset\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>\n",
    "                 Previously used: Medicare Part D Prescribers - by Provider and Drug |\n",
    "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                New dataset is available in datasets directory. which is real data of overall sales of medicines from different distributors and manufactures of Iran.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </P>\n",
    "\n",
    "    <p>\n",
    "        <label>\n",
    "            <strong>\n",
    "                CUDA\n",
    "            </strong>\n",
    "        </label>\n",
    "        <div>\n",
    "            For GPU Accelerated tasks: <br>\n",
    "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
    "            <br>\n",
    "            Then download supported spark-rapids jar file and add it to spark plugins. <br>\n",
    "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
    "            Spark rapids built at the top of cuDF, by installing them you can leverage GPU acceleration for Pandas as well without any code change.\n",
    "            <ul>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26fa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar\n"
     ]
    }
   ],
   "source": [
    "# CUDA toolkit Installation Instructions (Ubuntu):\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "# sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "# sudo apt-get update\n",
    "# sudo apt-get -y install cuda-toolkit-12-4\n",
    "\n",
    "%env SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b9d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark session...\n",
      "Session started\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "spark_rapids:str = os.getenv(\"SPARK_RAPIDS_PLUGIN_JAR\")\n",
    "\n",
    "# SPARK SESSION CONFIGURATION\n",
    "print(\"Running Spark session...\")\n",
    "session_builder = SparkSession.Builder()\n",
    "session_builder.master(\"local[*]\")\n",
    "session_builder.appName(\"Learning Spark\")\n",
    "session_builder.config(\"spark.ui.enabled\", True)\n",
    "session_builder.config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "session_builder.config(\"spark.ui.port\", \"8080\")\n",
    "session_builder.config(\"spark.driver.extraClassPath\", spark_rapids)\n",
    "session_builder.config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "session_builder.config(\"spark.rapids.memory.gpu.pooling.enabled\", True)\n",
    "session_builder.config(\"spark.rapids.sql.enabled\", True)\n",
    "session_builder.config(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "spark = session_builder.getOrCreate()\n",
    "print(\"Session started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f912d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Number of rows: 52445\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- manufacturer_name: string (nullable = true)\n",
      " |-- supplier_name: string (nullable = true)\n",
      " |-- brand_owner: string (nullable = true)\n",
      " |-- distributer_name: string (nullable = true)\n",
      " |-- origin_country: string (nullable = true)\n",
      " |-- drug_brand_fa: string (nullable = true)\n",
      " |-- drug_brand_en: string (nullable = true)\n",
      " |-- drug_generic_name: string (nullable = true)\n",
      " |-- active_ingredient: string (nullable = true)\n",
      " |-- total_box_sold: integer (nullable = true)\n",
      " |-- pills_count_in_box: integer (nullable = true)\n",
      " |-- total_sale_amount: integer (nullable = true)\n",
      " |-- total_sale_value_rial: double (nullable = true)\n",
      " |-- generic_code: integer (nullable = true)\n",
      " |-- otc: integer (nullable = true)\n",
      " |-- biologic: integer (nullable = true)\n",
      " |-- atc_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader, DataFrame\n",
    "\n",
    "\n",
    "dataset = amarnamehFile = \"datasets/amarnameh1400.csv\"\n",
    "\n",
    "def readData(format: str, fromPath: str):\n",
    "    print(\"Reading the dataset...\")\n",
    "    df_reader: DataFrameReader = spark.read\n",
    "    if format == \"csv\":\n",
    "        # inferSchema identifies the schema of provided file from it contents.\n",
    "        df: DataFrame = df_reader.csv(header=True, path=fromPath, inferSchema=True)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"parquet\":\n",
    "        df = df_reader.parquet(fromPath)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    return print(\"NOT SUPPORTED RIGHT NOW\")\n",
    "    \n",
    "# Converting csv file into parquet format.\n",
    "# Its optimized columnar storage and efficient compression\n",
    "def convertToParquet(df: DataFrame, write_path: str):\n",
    "    df = df.write.parquet(path=write_path, mode=\"ignore\")\n",
    "    return df\n",
    "\n",
    "spark_df = readData(\"csv\", amarnamehFile)\n",
    "df_cols = spark_df.columns\n",
    "renamed_list = [\"index\", \"manufacturer_name\", \"supplier_name\",\n",
    "                \"brand_owner\", \"distributer_name\", \"origin_country\",\n",
    "                \"drug_brand_fa\", \"drug_brand_en\", \"drug_generic_name\",\n",
    "                \"active_ingredient\", \"total_box_sold\", \"pills_count_in_box\",\n",
    "                \"total_sale_amount\",\"total_sale_value_rial\", \"generic_code\",\n",
    "                \"otc\", \"biologic\", \"atc_code\"]\n",
    "\n",
    "def columns_mapper(key: list, value: list) -> dict:\n",
    "    colsMap: dict = {}\n",
    "    for index in range(len(key)):\n",
    "        colsMap[key[index]] = value[index]\n",
    "    return colsMap\n",
    "\n",
    "colsMap = columns_mapper(df_cols, renamed_list)\n",
    "spark_df = spark_df.withColumnsRenamed(colsMap)\n",
    "\n",
    "print(f'Number of rows: {spark_df.count()}')\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fe2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drug manufacturers in Pharmaceutical market of Iran: 587\n"
     ]
    }
   ],
   "source": [
    "manufacturers = spark_df.groupby(\"manufacturer_name\").count().sort(\"count\", ascending=False)\n",
    "# NOTE: Duplications are not yet handled, some manufactures presents with several different names\n",
    "print(f\"Number of drug manufacturers in Pharmaceutical market of Iran: {manufacturers.count()}\")\n",
    "manufacturers.toPandas().to_excel(\"datasets/sorted_manufacturers.xlsx\", index=True, sheet_name=\"manufacturers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
