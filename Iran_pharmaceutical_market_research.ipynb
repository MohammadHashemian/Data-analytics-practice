{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38be22be",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Introduction to PySpark\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>Setup your working environment (Conda package management is recommended as it replace both pip and virtual environment)</li>\n",
       "            <li>I used WSL 2 for setting up a linux subsystem on my windows 11 in order to use GPU acceleration power.</li>\n",
       "            <li>You can install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
       "            <br> Don't forget to install Java SE 8 or above</li>\n",
       "            <li>I highly recommend to install spark at first, \n",
       "                then add \"SPARK_HOME\" environment variable in your ~/.bashrc and reload using \"source ~/.bashrc\", then install pyspark with conda.</li>\n",
       "            <li>If you want to open excel files, it's possible to open them with pandas with installed openpyxl as its dependency, then convert the file to csv, <br>\n",
       "                or instead there is an excel extension for spark 3.5.0 at the moment i work with the files. if your spark and scala version supports the jar file, open the excel file directly with spark.</li>\n",
       "        </ul>\n",
       "        <div>\n",
       "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
       "            I used Spark version 3.5.1 (Scala 2.13 & CUDA 12.5) and PySpark worked with python 3.11.9 well, earlier python version encountered with some errors during some code executions.\n",
       "        </div>\n",
       "    </P>\n",
       "</div>\n",
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Dataset\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>\n",
       "                 Previously used: Medicare Part D Prescribers - by Provider and Drug |\n",
       "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
       "            </li>\n",
       "            <li>\n",
       "                New dataset is available in datasets directory. which is real data of overall sales of medicines from different distributors and manufactures of Iran.\n",
       "            </li>\n",
       "        </ul>\n",
       "    </P>\n",
       "\n",
       "    <p>\n",
       "        <label>\n",
       "            <strong>\n",
       "                CUDA\n",
       "            </strong>\n",
       "        </label>\n",
       "        <div>\n",
       "            For GPU Accelerated tasks: <br>\n",
       "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
       "            <br>\n",
       "            Then download supported spark-rapids jar file and add it to spark plugins. <br>\n",
       "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
       "            Spark rapids built at the top of cuDF, by installing them you can leverage GPU acceleration for Pandas as well without any code change.\n",
       "            <ul>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
       "            </ul>\n",
       "        </div>\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Introduction to PySpark\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>Setup your working environment (Conda package management is recommended as it replace both pip and virtual environment)</li>\n",
    "            <li>I used WSL 2 for setting up a linux subsystem on my windows 11 in order to use GPU acceleration power.</li>\n",
    "            <li>You can install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
    "            <br> Don't forget to install Java SE 8 or above</li>\n",
    "            <li>I highly recommend to install spark at first, \n",
    "                then add \"SPARK_HOME\" environment variable in your ~/.bashrc and reload using \"source ~/.bashrc\", then install pyspark with conda.</li>\n",
    "            <li>If you want to open excel files, it's possible to open them with pandas with installed openpyxl as its dependency, then convert the file to csv, <br>\n",
    "                or instead there is an excel extension for spark 3.5.0 at the moment i work with the files. if your spark and scala version supports the jar file, open the excel file directly with spark.</li>\n",
    "        </ul>\n",
    "        <div>\n",
    "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
    "            I used Spark version 3.5.1 (Scala 2.13 & CUDA 12.5) and PySpark worked with python 3.11.9 well, earlier python version encountered with some errors during some code executions.\n",
    "        </div>\n",
    "    </P>\n",
    "</div>\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Dataset\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>\n",
    "                 Previously used: Medicare Part D Prescribers - by Provider and Drug |\n",
    "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                New dataset is available in datasets directory. which is real data of overall sales of medicines from different distributors and manufactures of Iran.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </P>\n",
    "\n",
    "    <p>\n",
    "        <label>\n",
    "            <strong>\n",
    "                CUDA\n",
    "            </strong>\n",
    "        </label>\n",
    "        <div>\n",
    "            For GPU Accelerated tasks: <br>\n",
    "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
    "            <br>\n",
    "            Then download supported spark-rapids jar file and add it to spark plugins. <br>\n",
    "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
    "            Spark rapids built at the top of cuDF, by installing them you can leverage GPU acceleration for Pandas as well without any code change.\n",
    "            <ul>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26fa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar\n"
     ]
    }
   ],
   "source": [
    "# CUDA toolkit Installation Instructions (Ubuntu):\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "# sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "# sudo apt-get update\n",
    "# sudo apt-get -y install cuda-toolkit-12-4\n",
    "\n",
    "%env SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b9d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark session...\n",
      "Session started\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "spark_rapids:str = os.getenv(\"SPARK_RAPIDS_PLUGIN_JAR\")\n",
    "\n",
    "# SPARK SESSION CONFIGURATION\n",
    "print(\"Running Spark session...\")\n",
    "session_builder = SparkSession.Builder() \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Learning Spark\") \\\n",
    "    .config(\"spark.ui.enabled\", True) \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.ui.port\", \"8080\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_rapids) \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.pooling.enabled\", True) \\\n",
    "    .config(\"spark.rapids.sql.enabled\", True) \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "spark = session_builder.getOrCreate()\n",
    "print(\"Session started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cca1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amarnamehFile = \"datasets/amarnameh1400.csv\"\n",
    "amarnamehParquet = \"datasets/amarnameh1400.parquet\"\n",
    "\n",
    "conversion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f912d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def readData(format: str, fromPath: str):\n",
    "    print(\"Reading the dataset...\")\n",
    "    if format == \"csv\":\n",
    "        # inferSchema identifies the schema of provided file from it contents.\n",
    "        df: DataFrame = spark.read.csv(header=True, path=fromPath, inferSchema=True)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"parquet\":\n",
    "        df = spark.read.parquet(fromPath)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"xlsx\":\n",
    "        print(\"Switching to Pandas, data frame reader\")\n",
    "        df = pd.read_excel(fromPath)\n",
    "        return df\n",
    "    return print(\"[ERROR] NOT SUPPORTED RIGHT NOW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3892c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataset...\n",
      "Completed\n",
      "Number of rows: 52445\n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- manufacturer_name: string (nullable = true)\n",
      " |-- supplier_name: string (nullable = true)\n",
      " |-- brand_owner: string (nullable = true)\n",
      " |-- distributer_name: string (nullable = true)\n",
      " |-- origin_country: string (nullable = true)\n",
      " |-- drug_brand_fa: string (nullable = true)\n",
      " |-- drug_brand_en: string (nullable = true)\n",
      " |-- drug_generic_name: string (nullable = true)\n",
      " |-- active_ingredient: string (nullable = true)\n",
      " |-- total_box_sold: integer (nullable = true)\n",
      " |-- pills_count_in_box: integer (nullable = true)\n",
      " |-- total_sale_amount: integer (nullable = true)\n",
      " |-- total_sale_value_rial: double (nullable = true)\n",
      " |-- generic_code: integer (nullable = true)\n",
      " |-- otc: integer (nullable = true)\n",
      " |-- biologic: integer (nullable = true)\n",
      " |-- atc_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_schema =   [\"index\", \"manufacturer_name\", \"supplier_name\",\n",
    "                \"brand_owner\", \"distributer_name\", \"origin_country\",\n",
    "                \"drug_brand_fa\", \"drug_brand_en\", \"drug_generic_name\",\n",
    "                \"active_ingredient\", \"total_box_sold\", \"pills_count_in_box\",\n",
    "                \"total_sale_amount\",\"total_sale_value_rial\", \"generic_code\",\n",
    "                \"otc\", \"biologic\", \"atc_code\"]\n",
    "\n",
    "# Optimized columnar storage and efficient compression\n",
    "def convertToParquet(df: DataFrame, write_path: str):\n",
    "    df = df.write.parquet(path=write_path, mode=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Renaming headers\n",
    "def columns_mapper(key: list, value: list) -> dict:\n",
    "    colsMap: dict = {}\n",
    "    for index in range(len(key)):\n",
    "        colsMap[key[index]] = value[index]\n",
    "    return colsMap\n",
    "\n",
    "# spark_df = readData(\"csv\", amarnamehFile)\n",
    "\n",
    "# df_cols = spark_df.columns\n",
    "# colsMap = columns_mapper(df_cols, renamed_list)\n",
    "# spark_df = spark_df.withColumnsRenamed(colsMap)\n",
    "\n",
    "# if conversion:\n",
    "#     convertToParquet(spark_df, \"datasets/amarnameh1400.parquet\")\n",
    "\n",
    "spark_df = readData(format=\"parquet\", fromPath=amarnamehParquet)\n",
    "print(f'Number of rows: {spark_df.count()}')\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fe2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drug manufacturers in Pharmaceutical market of Iran: 587\n",
      "+-----------------------------------+-----+\n",
      "|manufacturer_name                  |count|\n",
      "+-----------------------------------+-----+\n",
      "|Amin Pharmaceutical Co.            |2591 |\n",
      "|Darou Pakhsh                       |2188 |\n",
      "|Sobhandarou                        |2119 |\n",
      "|Raha Pharmaceutical Isfahan Company|1891 |\n",
      "|Aburaihan Pharmaceutical Co        |1782 |\n",
      "|Tehran Chemie Pharmaceutical Co.   |1674 |\n",
      "|Toliddaru Pharma. Co.              |1499 |\n",
      "|Exir Pharmaceutical Company        |1473 |\n",
      "|Caspian Tamin Co.                  |1418 |\n",
      "|Alborzdarou                        |1210 |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manufacturers = spark_df.groupby(\"manufacturer_name\").count().sort(\"count\", ascending=False)\n",
    "# NOTE: Duplications are not yet handled, some manufactures presents with several different names\n",
    "print(f\"Number of drug manufacturers in Pharmaceutical market of Iran: {manufacturers.count()}\")\n",
    "manufacturers.toPandas().to_excel(\"datasets/sorted_manufacturers.xlsx\", sheet_name=\"manufacturers\", index=True)\n",
    "manufacturers.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+--------+\n",
      "|manufacturer_name                         |count|language|\n",
      "+------------------------------------------+-----+--------+\n",
      "|Amin Pharmaceutical Co.                   |2591 |en      |\n",
      "|Darou Pakhsh                              |2188 |en      |\n",
      "|Sobhandarou                               |2119 |en      |\n",
      "|Raha Pharmaceutical Isfahan Company       |1891 |en      |\n",
      "|Aburaihan Pharmaceutical Co               |1782 |en      |\n",
      "|Tehran Chemie Pharmaceutical Co.          |1674 |en      |\n",
      "|Toliddaru Pharma. Co.                     |1499 |en      |\n",
      "|Exir Pharmaceutical Company               |1473 |en      |\n",
      "|Caspian Tamin Co.                         |1418 |en      |\n",
      "|Alborzdarou                               |1210 |en      |\n",
      "|Hakim Pharmaceutical Co.                  |1149 |en      |\n",
      "|Sina Darou Laboratories                   |1130 |en      |\n",
      "|Pars Darou                                |1028 |en      |\n",
      "|Daana Pharma                              |1019 |en      |\n",
      "|Razak Labratories                         |977  |en      |\n",
      "|Tehran Darou Pharmaceutical Co.           |879  |en      |\n",
      "|NO.115 , Jalal St ., Esteqlal Town ,Tehran|817  |en      |\n",
      "|Behvazan Pharmaceutical                   |816  |en      |\n",
      "|Chemidarou                                |814  |en      |\n",
      "|Farabi Pharmaceutical                     |778  |en      |\n",
      "+------------------------------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "words: tuple = (\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",\n",
    "         \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
    "         \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\",\n",
    "         \"v\", \"w\", \"x\", \"y\", \"z\")\n",
    "numbers: list = [str(n) for n in range(10)]\n",
    "numbers_tuple = tuple(numbers)\n",
    "\n",
    "# Simple way to differentiate written language of companies name \n",
    "def detect_language(name: str):\n",
    "    if name == None:\n",
    "        return \"null\"\n",
    "    if name.lower().startswith(words):\n",
    "        return \"en\"\n",
    "    elif name.lower().startswith(numbers_tuple):\n",
    "        return \"int\"\n",
    "    else:\n",
    "        return \"fa\"\n",
    "\n",
    "detect_lag = udf(detect_language, StringType())\n",
    "manufacturers = manufacturers.withColumn(\"language\", detect_lag(\"manufacturer_name\"))\n",
    "manufacturers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e33d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n",
      "|   manufacturer_name|count|language|\n",
      "+--------------------+-----+--------+\n",
      "|            میم دارو|   18|      fa|\n",
      "| نوشا فارمد ایرانیان|   10|      fa|\n",
      "|     نویا ویژن آریان|    5|      fa|\n",
      "|شرکت دارو سازی یا...|    5|      fa|\n",
      "|           توسن دارو|    3|      fa|\n",
      "|   نیواد فارمد سلامت|    2|      fa|\n",
      "|    زیست فناوری کوثر|    2|      fa|\n",
      "|فن آوری زیستی رزف...|    1|      fa|\n",
      "|          پارس تک رخ|    1|      fa|\n",
      "+--------------------+-----+--------+\n",
      "\n",
      "+-----------------+-----+--------+\n",
      "|manufacturer_name|count|language|\n",
      "+-----------------+-----+--------+\n",
      "|    3m Healthcare|    1|     int|\n",
      "+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manufacturers.filter(manufacturers.language == \"fa\").show()\n",
    "manufacturers.filter(manufacturers.language == \"int\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda15e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/projects/Data-analytics-practice/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: 344569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5012:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------------------------+--------+--------+--------------------+------------------+----------------+\n",
      "|manufacturer_name                 |manufacturer_name                  |soundex1|soundex2|levenshtein_distance|simple_fuzzy_ratio|similarity_score|\n",
      "+----------------------------------+-----------------------------------+--------+--------+--------------------+------------------+----------------+\n",
      "|novell pharmaceutical laboratories|novell pharmaceutical laboratories |N141    |N141    |0                   |100               |1.0             |\n",
      "|novell pharmaceutical laboratories|eli lilly                          |N141    |E444    |28                  |28                |0.14914232      |\n",
      "|novell pharmaceutical laboratories|doppel farmaceutici s.r.l          |N141    |D141    |19                  |54                |0.22473177      |\n",
      "|novell pharmaceutical laboratories|pharmachemie                       |N141    |P652    |25                  |43                |0.42596057      |\n",
      "|novell pharmaceutical laboratories|pannoc                             |N141    |P520    |31                  |15                |0.23565039      |\n",
      "|novell pharmaceutical laboratories|teb-mofid-nikan                    |N141    |T151    |30                  |16                |0.14852212      |\n",
      "|novell pharmaceutical laboratories|delpharm tours                     |N141    |D416    |23                  |50                |0.13462618      |\n",
      "|novell pharmaceutical laboratories|ges genricos espaoles s.a. (g.e.s.)|N141    |G225    |31                  |32                |0.15907924      |\n",
      "|novell pharmaceutical laboratories|alves healthcare private limited   |N141    |A412    |26                  |42                |0.20321366      |\n",
      "|novell pharmaceutical laboratories|octapharma                         |N141    |O231    |27                  |32                |0.030626964     |\n",
      "+----------------------------------+-----------------------------------+--------+--------+--------------------+------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, soundex, levenshtein, lower\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def fuzzy(company1, company2):\n",
    "    return fuzz.ratio(company1, company2)\n",
    "\n",
    "fuzzy_ratio = udf(fuzzy, IntegerType())\n",
    "\n",
    "def compute_similarity(company1, company2):\n",
    "    # Tokenize and encode the company names\n",
    "    inputs = tokenizer([company1, company2], return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Compute the mean of the embeddings to get a single vector for each input\n",
    "    company1_embedding = embeddings[0].mean(dim=0)\n",
    "    company2_embedding = embeddings[1].mean(dim=0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(company1_embedding, company2_embedding, dim=0)\n",
    "    \n",
    "    return cos_sim.item()\n",
    "\n",
    "# Register the UDF with Spark\n",
    "similarity_udf = udf(compute_similarity, FloatType())\n",
    "\n",
    "# Preparing for similarity checks\n",
    "manufacturers = manufacturers.withColumn(\"manufacturer_name\", lower(\"manufacturer_name\")) \\\n",
    "    .drop(\"count\", \"language\")\n",
    "\n",
    "manufacturers_coupled = manufacturers.alias(\"df1\").crossJoin(manufacturers.alias(\"df2\")) \\\n",
    "    .withColumn(\"soundex1\", soundex(col(\"df1.manufacturer_name\"))) \\\n",
    "    .withColumn(\"soundex2\", soundex(col(\"df2.manufacturer_name\"))) \\\n",
    "    .withColumn(\"levenshtein_distance\", levenshtein(col(\"df1.manufacturer_name\"), col(\"df2.manufacturer_name\"))) \\\n",
    "    .withColumn(\"simple_fuzzy_ratio\", fuzzy_ratio(col(\"df1.manufacturer_name\"), col(\"df2.manufacturer_name\"))) \\\n",
    "    .withColumn(\"similarity_score\", similarity_udf(col(\"df1.manufacturer_name\"), col(\"df2.manufacturer_name\"))) \\\n",
    "\n",
    "print(f\"Counts: {manufacturers_coupled.count()}\")\n",
    "manufacturers_coupled.show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b7b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# full_matches = manufacturers_coupled.filter(col(\"simple_fuzzy_ratio\") == i).show()\n",
    "    # .filter((col(\"soundex1\") == col(\"soundex2\")) & (col(\"levenshtein_distance\") > 0)) \\\n",
    "    # .select(col(\"df1.manufacturer_name\").alias(\"manufacturer_name_1\"),\n",
    "    #         col(\"df2.manufacturer_name\").alias(\"manufacturer_name_2\"),\n",
    "    #         col(\"soundex1\").alias(\"soundex\"),\n",
    "    #         col(\"levenshtein_distance\"),\n",
    "    #         col(\"similarity_score\"))\n",
    "# df_spark = df_spark.dropDuplicates([\"soundex\", \"levenshtein_distance\"]) \\\n",
    "#     .orderBy(col(\"similarity_score\").desc()) \\\n",
    "#     .filter(col(\"similarity_score\") >= float(0.39))\n",
    "# print(df_spark.count())\n",
    "# df_spark.show(truncate=False)\n",
    "\n",
    "# df_spark.toPandas().to_excel(\"datasets/sorted_manufacturer_lang.xlsx\", sheet_name=\"manufacturers\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
