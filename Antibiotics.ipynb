{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_RAPIDS_PLUGIN_JAR=/opt/spark/jars/rapids-4-spark_2.13-24.04.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark session...\n",
      "Session started\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "spark_rapids:str = os.getenv(\"SPARK_RAPIDS_PLUGIN_JAR\")\n",
    "\n",
    "# SPARK SESSION CONFIGURATION\n",
    "print(\"Running Spark session...\")\n",
    "session_builder = SparkSession.Builder() \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Learning Spark\") \\\n",
    "    .config(\"spark.ui.enabled\", True) \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.ui.port\", \"8080\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", spark_rapids) \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.pooling.enabled\", True) \\\n",
    "    .config(\"spark.rapids.sql.enabled\", True) \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"NONE\")\n",
    "spark = session_builder.getOrCreate()\n",
    "print(\"Session started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_excel = os.listdir(\"datasets-private/amarnameh-excel\")\n",
    "datasets_csv = sorted(os.listdir(\"datasets-private/amarnameh-csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- starting ----\n",
      "File: 1398.csv\n",
      "Reading the dataset...\n",
      "Completed\n",
      "---- Done ----\n",
      "---- starting ----\n",
      "File: 1399.csv\n",
      "Reading the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "---- Done ----\n",
      "---- starting ----\n",
      "File: 1400.csv\n",
      "Reading the dataset...\n",
      "Completed\n",
      "---- Done ----\n",
      "---- starting ----\n",
      "File: 1401.csv\n",
      "Reading the dataset...\n",
      "Completed\n",
      "---- Done ----\n",
      "---- starting ----\n",
      "File: 1402.csv\n",
      "Reading the dataset...\n",
      "Completed\n",
      "---- Done ----\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def readData(format: str, fromPath: str):\n",
    "    print(\"Reading the dataset...\")\n",
    "    if format == \"csv\":\n",
    "        # inferSchema identifies the schema of provided file from it contents.\n",
    "        df: DataFrame = spark.read.csv(header=True, path=fromPath, inferSchema=True)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"parquet\":\n",
    "        df = spark.read.parquet(fromPath)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"xlsx\":\n",
    "        print(\"Switching to Pandas, data frame reader\")\n",
    "        df = pd.read_excel(fromPath)\n",
    "        return df\n",
    "    return print(\"[ERROR] NOT SUPPORTED RIGHT NOW\")\n",
    "\n",
    "dataframes: dict[str, DataFrame] = {}\n",
    "for data in datasets_csv:\n",
    "    print(\"---- starting ----\")\n",
    "    print(f\"File: {str(data)}\")\n",
    "    filePath = f\"datasets-private/amarnameh-csv/{data}\"\n",
    "    df = readData(format=\"csv\", fromPath=filePath)\n",
    "    df_name = data.rstrip(\".csv\")\n",
    "    dataframes.update({df_name: df})\n",
    "    print(\"---- Done ----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset[1398]:\n",
      "-> Contains: 36999 Rows\n",
      "-> Headers Len: 27\n",
      "['تولید کننده / وارد کننده', 'صاحب پروانه', 'توزیع کننده', 'کد فرآورده', 'کد ATC', 'نام انگلیسی فرآورده', 'نام فارسی فرآورده', 'نام ژنریک', 'Column1', 'کد ژنریک', 'تولیدی/وارداتی', 'فوریتی/غیرفوریتی', 'نوع فرآورده', 'نام انگلیسی فهرست', 'کد ATC سطح1', 'عنوان ATC سطح 1', 'کد ATC سطح 2', 'عنوان ATC سطح 2', 'کد ATC سطح 3', 'عنوان ATC سطح 3', 'کد ATC سطح 4', 'عنوان ATC سطح 4', 'حجم فروش', 'تعداد در بسته', ' فروش عددی ', ' فروش ریالی ', '_c26']\n",
      "Dataset[1399]:\n",
      "-> Contains: 1048575 Rows\n",
      "-> Headers Len: 20\n",
      "['صاحب پروانه', 'توزیع کننده', 'IRC', 'نام فرآورده (برند)', 'تولیدی/وارداتی', 'OTC', 'بیولوژیک', 'کشور تولید کننده', 'تحت لیسانس', '111', 'نام ژنریک', 'فرآورده', 'کد ژنریک', 'ماده موثره', 'ATC Code', ' تعداد فروش (بسته) ', ' تعداد در بسته ', ' فروش عددی  ', ' فروش ریالی مصرف کننده  ', '_c19']\n",
      "Dataset[1400]:\n",
      "-> Contains: 52445 Rows\n",
      "-> Headers Len: 19\n",
      "['نام شرکت تولید کننده', 'نام شرکت تامین کننده', 'نام صاحب برند', 'توزیع کننده', 'کشور تولید  کننده', 'نام برند', 'نام لاتین برند', 'نام ژنریک', 'نام ماده موثره', ' تعداد فروش (بسته) ', ' تعداد در بسته ', ' فروش عددی ', ' فروش ریالی مصرف کننده ', 'کد ژنریک', 'OTC', 'بیولوژیک', 'ATC Code', '_c17', '_c18']\n",
      "Dataset[1401]:\n",
      "-> Contains: 125484 Rows\n",
      "-> Headers Len: 26\n",
      "['تولید کننده / وارد کننده', 'صاحب پروانه', 'توزیع کننده', 'کد فرآورده', 'کد ATC', 'نام انگلیسی فرآورده', 'نام فارسی فرآورده', 'کد ژنریک', 'نام ژنریک', 'تولیدی/وارداتی', 'فوریتی/غیرفوریتی', 'نوع فرآورده', 'نام انگلیسی فهرست', 'کد ATC سطح1', 'عنوان ATC سطح 1', 'کد ATC سطح 2', 'عنوان ATC سطح 2', 'کد ATC سطح 3', 'عنوان ATC سطح 3', 'کد ATC سطح 4', 'عنوان ATC سطح 4', 'حجم فروش', 'ارزش ریالی (مصرف کننده)', 'ارزش ریالی (توزیع کننده)', 'ارزش ریالی (داروخانه)', 'ماه شمسی']\n",
      "Dataset[1402]:\n",
      "-> Contains: 107213 Rows\n",
      "-> Headers Len: 26\n",
      "['تولید کننده / وارد کننده', 'صاحب پروانه', 'توزیع کننده', 'کد فرآورده', 'کد ATC', 'نام انگلیسی فرآورده', 'نام فارسی فرآورده', 'کد ژنریک', 'نام ژنریک', 'تولیدی/وارداتی', 'فوریتی/غیرفوریتی', 'نوع فرآورده', 'نام انگلیسی فهرست', 'کد ATC سطح1', 'عنوان ATC سطح 1', 'کد ATC سطح 2', 'عنوان ATC سطح 2', 'کد ATC سطح 3', 'عنوان ATC سطح 3', 'کد ATC سطح 4', 'عنوان ATC سطح 4', 'حجم فروش', 'ارزش ریالی (مصرف کننده)', 'ارزش ریالی (توزیع کننده)', 'ارزش ریالی (داروخانه)', 'ماه شمسی']\n"
     ]
    }
   ],
   "source": [
    "for df_name in dataframes:\n",
    "    df = dataframes.get(df_name)\n",
    "    # Rows number:\n",
    "    print(f\"Dataset[{df_name}]:\")\n",
    "    print(f'-> Contains: {df.count()} Rows')\n",
    "    print(f\"-> Headers Len: {len(df.columns)}\")\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing...\n"
     ]
    }
   ],
   "source": [
    "schema = [\"atc_code\", \"brand_owner\", \"drug_generic_name\"]\n",
    "\n",
    "\n",
    "print(\"Normalizing...\")\n",
    "df_1398 = dataframes.get(\"1398\")\n",
    "df_1399 = dataframes.get(\"1399\")\n",
    "df_1400 = dataframes.get(\"1400\")\n",
    "df_1401 = dataframes.get(\"1401\")\n",
    "df_1402 = dataframes.get(\"1402\")\n",
    "\n",
    "df_1398 = df_1398.withColumnsRenamed({\"صاحب پروانه\": \"brand_owner\", \"کد ATC\": \"atc_code\",}).select([\"brand_owner\", \"atc_code\"])\n",
    "df_1399 = df_1399.withColumnsRenamed({\"صاحب پروانه\":\"brand_owner\", \"ATC Code\": \"atc_code\"}).select([\"brand_owner\", \"atc_code\"])\n",
    "df_1400 = df_1400.withColumnsRenamed({\"نام صاحب برند\": \"brand_owner\", \"ATC Code\": \"atc_code\"}).select([\"brand_owner\",\"atc_code\"])\n",
    "df_1401 = df_1401.withColumnsRenamed({\"صاحب پروانه\" : \"brand_owner\", \"کد ATC\": \"atc_code\"}).select([\"brand_owner\", \"atc_code\"])\n",
    "df_1402 = df_1402.withColumnsRenamed({\"صاحب پروانه\" : \"brand_owner\", \"کد ATC\": \"atc_code\"}).select([\"brand_owner\", \"atc_code\"])\n",
    "dataframes.update({\"1398\": df_1398, \"1399\":df_1399,  \"1400\": df_1400, \"1401\": df_1401, \"1402\": df_1402})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataset...\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "antibiotics = readData(format=\"csv\", fromPath=\"datasets-private/antibiotics/ghotb-isfahan.csv\")\n",
    "antibiotics = antibiotics.select([\"ATC\", \"LABEL\"]).withColumnsRenamed(colsMap={\"ATC\": \"atc_code\",\n",
    "                                                                               \"LABEL\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame 1398 contains: 36999 rows\n",
      "All Antibiotics rows count: 1387\n",
      "All manufacturers with their ATC codes: 46\n",
      "-=-=-=-=-=-=-=-\n",
      "Data frame 1399 contains: 1048575 rows\n",
      "All Antibiotics rows count: 1459\n",
      "All manufacturers with their ATC codes: 42\n",
      "-=-=-=-=-=-=-=-\n",
      "Data frame 1400 contains: 52445 rows\n",
      "All Antibiotics rows count: 1338\n",
      "All manufacturers with their ATC codes: 40\n",
      "-=-=-=-=-=-=-=-\n",
      "Data frame 1401 contains: 125484 rows\n",
      "All Antibiotics rows count: 1223\n",
      "All manufacturers with their ATC codes: 42\n",
      "-=-=-=-=-=-=-=-\n",
      "Data frame 1402 contains: 107213 rows\n",
      "All Antibiotics rows count: 1087\n",
      "All manufacturers with their ATC codes: 42\n",
      "-=-=-=-=-=-=-=-\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "joined_dfs: dict[int, DataFrame] = {}\n",
    "result_dfs: dict[int, DataFrame] = {}\n",
    "\n",
    "def findManufacturers(data_frame: DataFrame):\n",
    "        joined_df = data_frame \\\n",
    "            .join(antibiotics, on=\"atc_code\", how=\"inner\") \\\n",
    "            .dropDuplicates([\"brand_owner\", \"label\"]).sort(\"label\")\n",
    "        return joined_df\n",
    "\n",
    "def countManufacturers(dataframes: dict, start: int, stop: int):\n",
    "    for i in range(start, stop):\n",
    "        df: DataFrame = dataframes.get(str(i))\n",
    "        print(f\"Data frame {i} contains: {df.count()} rows\")\n",
    "\n",
    "        joined_df: DataFrame = findManufacturers(df)\n",
    "        joined_dfs.update(  {i: joined_df}  )\n",
    "        print(f\"All Antibiotics rows count: {joined_df.count()}\")\n",
    "\n",
    "        result_df = joined_df.groupBy(\"atc_code\").agg(countDistinct(\"brand_owner\").alias(f\"manufacturers_{i}\"))\n",
    "        print(f\"All manufacturers with their ATC codes: {result_df.count()}\")\n",
    "        print(\"-=-=-=-=-=-=-=-\")\n",
    "        result_dfs.update(  {i: result_df}  )\n",
    "\n",
    "countManufacturers(dataframes=dataframes, start=1398, stop=1403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final result...\n",
      "result page ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined data frames pages ...\n",
      "page 1398 ...\n",
      "page 1399 ...\n",
      "page 1400 ...\n",
      "page 1401 ...\n",
      "page 1402 ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "final_df = result_dfs.get(1398).join(result_dfs.get(1399), on=\"atc_code\", how=\"outer\") \\\n",
    ".join(result_dfs.get(1400), on=\"atc_code\", how=\"outer\") \\\n",
    ".join(result_dfs.get(1401), on=\"atc_code\", how=\"outer\") \\\n",
    ".join(result_dfs.get(1402), on=\"atc_code\", how=\"outer\") \\\n",
    ".join(antibiotics, on=\"atc_code\", how=\"inner\")\n",
    "\n",
    "print(\"Saving final result...\")\n",
    "with pd.ExcelWriter(\"datasets-private/result.xlsx\") as writer:\n",
    "    print(\"result page ...\")\n",
    "    pd = final_df.toPandas().to_excel(writer, sheet_name=\"result\", index=True)\n",
    "    \n",
    "    print(\"joined data frames pages ...\")\n",
    "    for i in range(1398, 1403):\n",
    "        print(f\"page {i} ...\")\n",
    "        joined_dfs.get(i).toPandas().to_excel(writer, sheet_name=f\"{i}\", index=False)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
