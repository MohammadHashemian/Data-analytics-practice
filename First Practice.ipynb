{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38be22be",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Introduction to PySpark\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>Setup your working environment (Jupyter notebook is recommended)</li>\n",
       "            <li>Install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
       "            <br> Don't forget to install Java SE 8 or above</li>\n",
       "        </ul>\n",
       "        <div>\n",
       "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
       "            PySpark works with Python 3.11 or older otherwise you may encounter some issues like me.\n",
       "        </div>\n",
       "    </P>\n",
       "</div>\n",
       "<div>\n",
       "    <P>\n",
       "        <label>\n",
       "            <strong>\n",
       "            Dataset\n",
       "            </strong>\n",
       "        </label>\n",
       "        <ul>\n",
       "            <li>\n",
       "                Medicare Part D Prescribers - by Provider and Drug |\n",
       "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
       "            </li>\n",
       "        </ul>\n",
       "    </P>\n",
       "\n",
       "    <p>\n",
       "        <label>\n",
       "            <strong>\n",
       "                CUDA\n",
       "            </strong>\n",
       "        </label>\n",
       "        <div>\n",
       "            For GPU Accelerated tasks:\n",
       "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
       "            <ul>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
       "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
       "            </ul>\n",
       "        </div>\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Introduction to PySpark\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>Setup your working environment (Jupyter notebook is recommended)</li>\n",
    "            <li>Install PySpark from provided Instruction. <a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">here</a>\n",
    "            <br> Don't forget to install Java SE 8 or above</li>\n",
    "        </ul>\n",
    "        <div>\n",
    "            <a style=\"font-weight: 500; color: red;\">Note:</a>\n",
    "            PySpark works with Python 3.11 or older otherwise you may encounter some issues like me.\n",
    "        </div>\n",
    "    </P>\n",
    "</div>\n",
    "<div>\n",
    "    <P>\n",
    "        <label>\n",
    "            <strong>\n",
    "            Dataset\n",
    "            </strong>\n",
    "        </label>\n",
    "        <ul>\n",
    "            <li>\n",
    "                Medicare Part D Prescribers - by Provider and Drug |\n",
    "                <a href=\"https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug\"> Download </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </P>\n",
    "\n",
    "    <p>\n",
    "        <label>\n",
    "            <strong>\n",
    "                CUDA\n",
    "            </strong>\n",
    "        </label>\n",
    "        <div>\n",
    "            For GPU Accelerated tasks:\n",
    "            Install or update CUDA toolkit if your nvidia graphic card supports.\n",
    "            <ul>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-gpus\">Supported Graphic cards</a></li>\n",
    "            <li><a href=\"https://developer.nvidia.com/cuda-toolkit\">Download</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26fa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA toolkit Installation Instructions (Ubuntu):\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "# sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "# wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.1-550.54.15-1_amd64.deb\n",
    "# sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "# sudo apt-get update\n",
    "# sudo apt-get -y install cuda-toolkit-12-4\n",
    "\n",
    "\n",
    "# It is essential to provide the address of SPARK RAPIDS Plugin for Spark\n",
    "# Using environment variable can be a nice option to provide them.\n",
    "# %env SPARK_RAPIDS_DIR=\n",
    "# %env SPARK_RAPIDS_PLUGIN_JAR="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b9d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m session_builder\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.memory.gpu.pooling.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m session_builder\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.rapids.sql.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43msession_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Renaming our variable name for better readability\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Projects\\Data-analytics-practice\\venv\\Lib\\site-packages\\py4j\\protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark_rapids:str = os.getenv(\"SPARK_RAPIDS_PLUGIN_JAR\")\n",
    "\n",
    "# Creating Spark session to start using spark\n",
    "# When you are working in clouds you can create multiple instances\n",
    "print(\"Running Spark session...\")\n",
    "session_builder = SparkSession.Builder()\n",
    "session_builder.appName(\"First Practice\")\n",
    "session_builder.master(\"local[*]\")\n",
    "session_builder.config(\"spark.ui.enabled\", True)\n",
    "session_builder.config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "session_builder.config(\"spark.ui.port\", \"8080\")\n",
    "# Make sure you are running on a Linux Operation system, otherwise spark rapids not supported\n",
    "session_builder.config(\"spark.driver.extraClassPath\", spark_rapids)\n",
    "session_builder.config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "session_builder.config(\"spark.rapids.memory.gpu.pooling.enabled\", True)\n",
    "session_builder.config(\"spark.rapids.sql.enabled\", True)\n",
    "session = session_builder.getOrCreate()\n",
    "print(\"Session started\")\n",
    "\n",
    "# Renaming our variable name for better readability\n",
    "spark = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f912d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameReader, DataFrame\n",
    "\n",
    "\n",
    "file_path = \"datasets/Medicare-PD/MUP_DPR_RY23_P04_V10_DY21_NPIBN.csv\"\n",
    "parquet_path = \"datasets/Medicare-PD-Parquet\"\n",
    "\n",
    "def readData(format: str, fromPath: str):\n",
    "    print(\"Reading the dataset...\")\n",
    "    df_reader: DataFrameReader = spark.read\n",
    "    if format == \"csv\":\n",
    "        # Reading the data from csv file.\n",
    "        # inferSchema idenifies the schema of provided file from it contents.\n",
    "        df: DataFrame = df_reader.csv(header=True, path=fromPath, inferSchema=True)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    if format == \"parquet\":\n",
    "        df = df_reader.parquet(fromPath)\n",
    "        print(\"Completed\")\n",
    "        return df\n",
    "    return print(\"NOT SUPPORTED RIGHT NOW\")\n",
    "    \n",
    "# Converting csv file into parquet format.\n",
    "# Its optimized columnar storage and efficient compression\n",
    "def convertToParquet(df: DataFrame, write_path: str):\n",
    "    df = df.write.parquet(path=write_path, mode=\"ignore\")\n",
    "    return df\n",
    "\n",
    "# Follow line executed once, converting 3.5Gb csv file into 520Mb parquet\n",
    "# csv_df = readData(format=\"csv\", filePath=file_path)\n",
    "# df = convertToParquet(csv_df, parquet_path)\n",
    "# csv.printSchema() is Deprecated\n",
    "\n",
    "df = readData(format=\"parquet\", fromPath=parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fe2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Prscrbr_NPI: integer (nullable = true)\n",
      " |-- Prscrbr_Last_Org_Name: string (nullable = true)\n",
      " |-- Prscrbr_First_Name: string (nullable = true)\n",
      " |-- Prscrbr_City: string (nullable = true)\n",
      " |-- Prscrbr_State_Abrvtn: string (nullable = true)\n",
      " |-- Prscrbr_State_FIPS: string (nullable = true)\n",
      " |-- Prscrbr_Type: string (nullable = true)\n",
      " |-- Prscrbr_Type_Src: string (nullable = true)\n",
      " |-- Brnd_Name: string (nullable = true)\n",
      " |-- Gnrc_Name: string (nullable = true)\n",
      " |-- Tot_Clms: integer (nullable = true)\n",
      " |-- Tot_30day_Fills: double (nullable = true)\n",
      " |-- Tot_Day_Suply: integer (nullable = true)\n",
      " |-- Tot_Drug_Cst: double (nullable = true)\n",
      " |-- Tot_Benes: integer (nullable = true)\n",
      " |-- GE65_Sprsn_Flag: string (nullable = true)\n",
      " |-- GE65_Tot_Clms: integer (nullable = true)\n",
      " |-- GE65_Tot_30day_Fills: double (nullable = true)\n",
      " |-- GE65_Tot_Drug_Cst: double (nullable = true)\n",
      " |-- GE65_Tot_Day_Suply: integer (nullable = true)\n",
      " |-- GE65_Bene_Sprsn_Flag: string (nullable = true)\n",
      " |-- GE65_Tot_Benes: integer (nullable = true)\n",
      "\n",
      "Number of rows: 25231862\n"
     ]
    }
   ],
   "source": [
    "# Getting the schema of the dataset\n",
    "df.printSchema()\n",
    "print(f'Number of rows: {df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91201796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is much slower\n",
    "# Reading the dataframe from it may cause out-off-memory error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
